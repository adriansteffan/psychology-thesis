@software{apachesoftwarefoundationApacheHTTPServer2012,
  title = {Apache {{HTTP Server}}},
  author = {{Apache Software Foundation,}},
  date = {2012},
  url = {https://httpd.apache.org/},
  version = {2.4}
}

@article{bankiComparingOnlineWebcam2022a,
  title = {Comparing {{Online Webcam-}} and {{Laboratory-Based Eye-Tracking}} for the {{Assessment}} of {{Infants}}’ {{Audio-Visual Synchrony Perception}}},
  author = {Bánki, Anna and family=Eccher, given=Martina, prefix=de, useprefix=true and Falschlehner, Lilith and Hoehl, Stefanie and Markova, Gabriela},
  date = {2022},
  journaltitle = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.733933},
  urldate = {2022-10-31},
  abstract = {Online data collection with infants raises special opportunities and challenges for developmental research. One of the most prevalent methods in infancy research is eye-tracking, which has been widely applied in laboratory settings to assess cognitive development. Technological advances now allow conducting eye-tracking online with various populations, including infants. However, the accuracy and reliability of online infant eye-tracking remain to be comprehensively evaluated. No research to date has directly compared webcam-based and in-lab eye-tracking data from infants, similarly to data from adults. The present study provides a direct comparison of in-lab and webcam-based eye-tracking data from infants who completed an identical looking time paradigm in two different settings (in the laboratory or online at home). We assessed 4-6-month-old infants (n = 38) in an eye-tracking task that measured the detection of audio-visual asynchrony. Webcam-based and in-lab eye-tracking data were compared on eye-tracking and video data quality, infants’ viewing behavior, and experimental effects. Results revealed no differences between the in-lab and online setting in the frequency of technical issues and participant attrition rates. Video data quality was comparable between settings in terms of completeness and brightness, despite lower frame rate and resolution online. Eye-tracking data quality was higher in the laboratory than online, except in case of relative sample loss. Gaze data quantity recorded by eye-tracking was significantly lower than by video in both settings. In valid trials, eye-tracking and video data captured infants’ viewing behavior uniformly, irrespective of setting. Despite the common challenges of infant eye-tracking across experimental settings, our results point toward the necessity to further improve the precision of online eye-tracking with infants. Taken together, online eye-tracking is a promising tool to assess infants’ gaze behavior but requires careful data quality control. The demographic composition of both samples differed from the generic population on caregiver education: our samples comprised caregivers with higher-than-average education levels, challenging the notion that online studies will per se reach more diverse populations.},
  file = {/Users/adriansteffan/Zotero/storage/Y43LG8L3/Bánki et al. - 2022 - Comparing Online Webcam- and Laboratory-Based Eye-.pdf}
}

@article{bottWebCameraBased2017,
  title = {Web {{Camera Based Eye Tracking}} to {{Assess Visual Memory}} on a {{Visual Paired Comparison Task}}},
  author = {Bott, Nicholas T. and Lange, Alex and Rentz, Dorene and Buffalo, Elizabeth and Clopton, Paul and Zola, Stuart},
  date = {2017},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {11},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00370},
  url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00370/full},
  urldate = {2021-05-08},
  abstract = {Background: Web cameras are increasingly part of the standard hardware of most smart devices. Eye movements can often provide a noninvasive “window on the brain”, and the recording of eye movements using web cameras is a burgeoning area of research. Objective: This study investigated a novel methodology for administering a visual paired comparison (VPC) decisional task using a web camera. To further assess this method, we examined the correlation between a standard eye-tracking camera automated scoring procedure (obtaining images at 60 frames per second [FPS]) and a manually scored procedure using a built-in laptop web camera (obtaining images at 3 FPS). Methods: This was an observational study of 54 clinically normal older adults. Subjects completed three in-clinic visits with simultaneous recording of eye movements on a VPC decision task by a standard eye tracker camera and a built-in laptop-based web camera. Inter-rater reliability was analyzed using Siegel and Castellan’s kappa formula. Pearson correlations were used to investigate the correlation between VPC performance using a standard eye tracker camera and a built-in web camera. Results: Strong associations were observed on VPC mean novelty preference score between the 60 FPS eye tracker and 3 FPS built-in web camera at each of the three visits (r = 0.88 – 0.92). Inter-rater agreement of web camera scoring at each time point was high (κ = 0.81 to 0.88). There were strong relationships on VPC mean novelty preference score between 10 FPS, 5 FPS, and 3 FPS training sets (r = 0.88 to 0.94). Significantly fewer data quality issues were encountered using the built-in web camera. Conclusions: Human scoring of a VPC decisional task using a built-in laptop web camera correlated strongly with automated scoring of the same task using a standard high frame rate eye tracker camera. While this method is not suitable for eye tracking paradigms requiring the collection and analysis of fine-grained metrics such as fixation points, built-in web cameras are a standard feature of most smart devices (e.g., laptops, tablets, smart phones) and can be effectively employed to track eye movements on decisional tasks with high accuracy and minimal cost.},
  langid = {english},
  keywords = {eye tracking,methodology comparison,visual memory,Visual paired comparison,Web-Camera},
  file = {/Users/adriansteffan/Zotero/storage/M6VYGJYU/Bott et al. - 2017 - Web Camera Based Eye Tracking to Assess Visual Mem.pdf}
}

@report{cairoImpactCodeSmells2018,
  type = {preprint},
  title = {The {{Impact}} of {{Code Smells}} on {{Software Bugs}}: A {{Systematic Literature Review}}},
  shorttitle = {The {{Impact}} of {{Code Smells}} on {{Software Bugs}}},
  author = {Cairo, Aloisio and Carneiro, Glauco and Monteiro, Miguel},
  date = {2018-10-03},
  institution = {{MATHEMATICS \& COMPUTER SCIENCE}},
  doi = {10.20944/preprints201810.0059.v1},
  url = {https://www.preprints.org/manuscript/201810.0059/v1},
  urldate = {2023-07-03},
  abstract = {Context: Code smells are associated with poor design and programming style that often degrades code quality and hampers code comprehensibility and maintainability. Goal: Identify reports from the literature that provide evidence of the influence of code smells on the occurrence of software bugs. Method: We conducted a Systematic Literature Review (SLR) to reach the stated goal. Results: The SLR includes selected studies from July 2007 to September 2017 which analyzed the source code for open source and proprietary projects, as well, as several code smells and anti-patterns. The results of this SLR show that 24 code smells are more influential in the occurrence of bugs according to 16 studies. In contrast, three studies reported that at least 6 code smells are less influential in such occurrences. Evidence from the selected studies also point out tools, techniques and procedures applied to analyze the influence. Conclusion: To the best of our knowledge, this is the first SLR to target this goal. This study provides an up-to-date and structured understanding of the influence of code smells on the occurrence of software bugs based on findings systematically collected from a list of relevant references in the latest decade.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/UPH989VE/Cairo et al. - 2018 - The Impact of Code Smells on Software Bugs a Syst.pdf}
}

@software{canonicalltd.Ubuntu18042018,
  title = {Ubuntu 18.04 {{LTS}}},
  author = {{Canonical Ltd.}},
  date = {2018},
  url = {https://ubuntu.com/}
}

@online{chueyConductingDevelopmentalResearch2022,
  title = {Conducting Developmental Research Online vs. in-Person: {{A}} Meta-Analysis},
  shorttitle = {Conducting Developmental Research Online vs. in-Person},
  author = {Chuey, Aaron and Boyce, Veronica and Cao, Anjie and Frank, Michael C.},
  date = {2022-11-16T01:15:06},
  doi = {10.31234/osf.io/qc6fw},
  url = {https://psyarxiv.com/qc6fw/},
  urldate = {2023-07-03},
  abstract = {An increasing number of psychological experiments with children are being conducted using online platforms, in part due to the COVID-19 pandemic. Individual replications have compared the findings of particular experiments online and in-person, but the general effect of online data collection on data collected from children is still unknown. Therefore, the current meta-analysis examines how the effect sizes of developmental studies conducted online compare to the same studies conducted in-person. Our pre-registered analysis includes 145 effect sizes calculated from 24 papers with 2440 children, ranging in age from four months to six years. We examined several moderators of the effect of online testing, including the role of dependent measure (looking vs verbal), online study method (moderated vs unmoderated), and age. The mean effect size of studies conducted in-person (d = .68) was slightly larger than the mean effect size of their counterparts conducted online (d = .54), but this difference was not significant. Additionally, we found no significant moderating effect of dependent measure, online study method, or age. Overall, the results of the current meta-analysis suggest developmental data collected online are generally comparable to data collected in-person.},
  langid = {american},
  pubstate = {preprint},
  keywords = {Cognitive Development,Development,Developmental Psychology,Meta-analysis,Methodology,Online studies,Quantitative Methods,Social and Behavioral Sciences},
  file = {/Users/adriansteffan/Zotero/storage/FHKFYUWY/Chuey et al. - 2022 - Conducting developmental research online vs. in-pe.pdf}
}

@article{chueyModeratedOnlineDataCollection2021,
  title = {Moderated {{Online Data-Collection}} for {{Developmental Research}}: {{Methods}} and {{Replications}}},
  shorttitle = {Moderated {{Online Data-Collection}} for {{Developmental Research}}},
  author = {Chuey, Aaron and Asaba, Mika and Bridgers, Sophie and Carrillo, Brandon and Dietz, Griffin and Garcia, Teresa and Leonard, Julia A. and Liu, Shari and Merrick, Megan and Radwan, Samaher and Stegall, Jessa and Velez, Natalia and Woo, Brandon and Wu, Yang and Zhou, Xi J. and Frank, Michael C. and Gweon, Hyowon},
  date = {2021},
  journaltitle = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.734398},
  urldate = {2023-07-03},
  abstract = {Online data collection methods are expanding the ease and access of developmental research for researchers and participants alike. While its popularity among developmental scientists has soared during the COVID-19 pandemic, its potential goes beyond just a means for safe, socially distanced data collection. In particular, advances in video conferencing software has enabled researchers to engage in face-to-face interactions with participants from nearly any location at any time. Due to the novelty of these methods, however, many researchers still remain uncertain about the differences in available approaches as well as the validity of online methods more broadly. In this article, we aim to address both issues with a focus on moderated (synchronous) data collected using video-conferencing software (e.g., Zoom). First, we review existing approaches for designing and executing moderated online studies with young children. We also present concrete examples of studies that implemented choice and verbal measures (Studies 1 and 2) and looking time (Studies 3 and 4) across both in-person and online moderated data collection methods. Direct comparison of the two methods within each study as well as a meta-analysis of all studies suggest that the results from the two methods are comparable, providing empirical support for the validity of moderated online data collection. Finally, we discuss current limitations of online data collection and possible solutions, as well as its potential to increase the accessibility, diversity, and replicability of developmental science.},
  file = {/Users/adriansteffan/Zotero/storage/BH2L24S5/Chuey et al. - 2021 - Moderated Online Data-Collection for Developmental.pdf}
}

@article{dalrympleExaminationRecordingAccuracy2018,
  title = {An {{Examination}} of {{Recording Accuracy}} and {{Precision From Eye Tracking Data From Toddlerhood}} to {{Adulthood}}},
  author = {Dalrymple, Kirsten A. and Manner, Marie D. and Harmelink, Katherine A. and Teska, Elayne P. and Elison, Jed T.},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00803},
  urldate = {2023-07-03},
  abstract = {The quantitative assessment of eye tracking data quality is critical for ensuring accuracy and precision of gaze position measurements. However, researchers often report the eye tracker’s optimal manufacturer’s specifications rather than empirical data about the accuracy and precision of the eye tracking data being presented. Indeed, a recent report indicates that less than half of eye tracking researchers surveyed take the eye tracker’s accuracy into account when determining areas of interest for analysis, an oversight that could impact the validity of reported results and conclusions. Accordingly, we designed a calibration verification protocol to augment independent quality assessment of eye tracking data and examined whether accuracy and precision varied between three age groups of participants. We also examined the degree to which our externally quantified quality assurance metrics aligned with those reported by the manufacturer. We collected data in standard laboratory conditions to demonstrate our method, to illustrate how data quality can vary with participant age, and to give a simple example of the degree to which data quality can differ from manufacturer reported values. In the sample data we collected, accuracy for adults was within the range advertised by the manufacturer, but for school-aged children, accuracy and precision measures were outside this range. Data from toddlers were less accurate and less precise than data from adults. Based on an a priori inclusion criterion, we determined that we could exclude approximately 20\% of toddler participants for poor calibration quality quantified using our calibration assessment protocol. We recommend implementing and reporting quality assessment protocols for any eye tracking tasks with participants of any age or developmental ability. We conclude with general observations about our data, recommendations for what factors to consider when establishing data inclusion criteria, and suggestions for stimulus design that can help accommodate variability in calibration. The methods outlined here may be particularly useful for developmental psychologists who use eye tracking as a tool, but who are not experts in eye tracking per se. The calibration verification stimuli and data processing scripts that we developed, along with step-by-step instructions, are freely available for other researchers.},
  file = {/Users/adriansteffan/Zotero/storage/UJ5AZ8LA/Dalrymple et al. - 2018 - An Examination of Recording Accuracy and Precision.pdf}
}

@article{deleeuwJsPsychJavaScriptLibrary2015,
  title = {{{jsPsych}}: {{A JavaScript}} Library for Creating Behavioral Experiments in a {{Web}} Browser},
  shorttitle = {{{jsPsych}}},
  author = {family=Leeuw, given=Joshua R., prefix=de, useprefix=true},
  date = {2015-03},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {47},
  number = {1},
  pages = {1--12},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0458-y},
  url = {http://link.springer.com/10.3758/s13428-014-0458-y},
  urldate = {2021-05-13},
  abstract = {Online experiments are growing in popularity, and the increasing sophistication of Web technology has made it possible to run complex behavioral experiments online using only a Web browser. Unlike with offline laboratory experiments, however, few tools exist to aid in the development of browser-based experiments. This makes the process of creating an experiment slow and challenging, particularly for researchers who lack a Web development background. This article introduces jsPsych, a JavaScript library for the development of Web-based experiments. jsPsych formalizes a way of describing experiments that is much simpler than writing the entire experiment from scratch. jsPsych then executes these descriptions automatically, handling the flow from one task to another. The jsPsych library is opensource and designed to be expanded by the research community. The project is available online at www.jspsych.org.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/PMQ4PPLQ/PMQ4PPLQ.pdf}
}

@article{erelICatcherNeuralNetwork2022a,
  title = {{{iCatcher}}: {{A}} Neural Network Approach for Automated Coding of Young Children's Eye Movements},
  shorttitle = {{{iCatcher}}},
  author = {Erel, Yotam and Potter, Christine E. and Jaffe-Dax, Sagi and Lew-Williams, Casey and Bermano, Amit H.},
  date = {2022},
  journaltitle = {Infancy},
  volume = {27},
  number = {4},
  pages = {765--779},
  issn = {1532-7078},
  doi = {10.1111/infa.12468},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/infa.12468},
  urldate = {2023-07-03},
  abstract = {Infants' looking behaviors are often used for measuring attention, real-time processing, and learning—often using low-resolution videos. Despite the ubiquity of gaze-related methods in developmental science, current analysis techniques usually involve laborious post hoc coding, imprecise real-time coding, or expensive eye trackers that may increase data loss and require a calibration phase. As an alternative, we propose using computer vision methods to perform automatic gaze estimation from low-resolution videos. At the core of our approach is a neural network that classifies gaze directions in real time. We compared our method, called iCatcher, to manually annotated videos from a prior study in which infants looked at one of two pictures on a screen. We demonstrated that the accuracy of iCatcher approximates that of human annotators and that it replicates the prior study's results. Our method is publicly available as an open-source repository at https://github.com/yoterel/iCatcher.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/6TSZQGMI/Erel et al. - 2022 - iCatcher A neural network approach for automated .pdf;/Users/adriansteffan/Zotero/storage/Q43ZKD43/infa.html}
}

@online{erelICatcherRobustAutomated2022a,
  title = {{{iCatcher}}+: {{Robust}} and Automated Annotation of Infant's and Young Children's Gaze Direction from Videos Collected in Laboratory, Field, and Online Studies},
  shorttitle = {{{iCatcher}}+},
  author = {Erel, Yotam and Shannon, Kat Adams and Chu, Junyi and Scott, Kim and Struhl, Melissa Kline and Cao, Peng and Tan, Xincheng and Hart, Peter K. and Raz, Gal and Piccolo, Sabrina and Mei, Catherine and Potter, Christine and Jaffe-Dax, Sagi and Lew-Williams, Casey and Tenenbaum, Joshua and Fairchild, Katherine and Bermano, Amit and Liu, Shari},
  date = {2022-04-30T21:51:59},
  doi = {10.31234/osf.io/up97k},
  url = {https://psyarxiv.com/up97k/},
  urldate = {2023-07-03},
  abstract = {Technological advances in psychological research have enabled large-scale studies of human behavior and streamlined pipelines for automatic processing of data. However, studies of infants and children have not fully reaped these benefits, because the behaviors of interest, such as gaze duration and direction, even when collected online, still have to be extracted from video through a laborious process of manual annotation. Recent advances in computer vision raise the possibility of automated annotation of this video data. In this paper, we built on a system for automatic gaze annotation in young children, iCatcher (Erel et al., 2022), by engineering improvements, and then training and testing the system (hereafter, iCatcher+) on three datasets with substantial video and participant variability (214 videos collected in United States lab and field sites, 143 videos collected in Senegal field sites, and 265 videos collected via webcams in homes; participants aged 4 months-3.5 years). When trained on each of these datasets, iCatcher+ performed with near human-level accuracy on held-out videos on distinguishing “LEFT” versus “RIGHT”, and “ON” versus “OFF” looking behavior, across all datasets. This high performance was achieved at the level of individual frames, experimental trials, and study videos, held across participant demographics (e.g., age, race/ethnicity), participant behavior (e.g., movement, head position) and video characteristics (e.g., luminance), and generalized to a fourth, entirely held-out online dataset. We close by discussing next steps required to fully automate the lifecycle of online infant and child behavioral studies, representing a key step towards enabling rapid, high-powered developmental research.},
  langid = {american},
  pubstate = {preprint},
  keywords = {cognitive development,Cognitive Psychology,deep learning,Developmental Psychology,eye-tracking,online data collection,open source,Quantitative Methods,Social and Behavioral Sciences},
  file = {/Users/adriansteffan/Zotero/storage/MZCMMG5A/Erel et al. - 2022 - iCatcher+ Robust and automated annotation of infa.pdf}
}

@incollection{fernaldLookingListeningUsing2008,
  title = {Looking While Listening: {{Using}} Eye Movements to Monitor Spoken Language Comprehension by Infants and Young Children},
  shorttitle = {Looking While Listening},
  booktitle = {Developmental Psycholinguistics: {{On-line}} Methods in Children's Language Processing},
  author = {Fernald, Anne and Zangl, Renate and Portillo, Ana Luz and Marchman, Virginia A.},
  date = {2008},
  series = {Language Acquisition and Language Disorders},
  pages = {97--135},
  publisher = {{John Benjamins Publishing Company}},
  location = {{Amsterdam, Netherlands}},
  doi = {10.1075/lald.44.06fer},
  url = {https://doi.org/10.1075/lald.44.06fer},
  abstract = {The "looking-while-listening" methodology uses real-time measures of the time course of young children's gaze patterns in response to speech. This procedure is low in task demands and does not require automated eyetracking technology, similar to "preferential-looking" procedures. However, the looking-while-listening methodology differs critically from preferential-looking procedures in the methods used for data reduction and analysis, yielding high-resolution measures of speech processing from moment to moment, rather than relying on summary measures of looking preference. Because children's gaze patterns are time-locked to speech and coded frame-by-frame, each 5-min experiment response latencies can be coded with millisecond precision on multiple trials over multiple items, based on data from thousands of frames in each experiment. The meticulous procedures required in the collection, reduction, and multiple levels of analysis of such detailed data are demanding, but well worth the effort, revealing a dynamic and nuanced picture of young children's developing skill in finding meaning in spoken language. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-90-272-5305-7 978-90-272-5304-0},
  keywords = {Eye Movements,Language Development,Methodology,Physiological Correlates,Speech Perception,Verbal Comprehension},
  file = {/Users/adriansteffan/Zotero/storage/ZBQ72QZE/2007-18520-004.html}
}

@inproceedings{goekeLabVancedUnifiedJavaScript2017,
  title = {{{LabVanced}}: {{A Unified JavaScript Framework}} for {{Online Studies}}},
  shorttitle = {{{LabVanced}}},
  author = {Goeke, Caspar and Finger, Holger and Diekamp, Dorena and Standvoss, Kai and König, Peter},
  date = {2017-07-10},
  file = {/Users/adriansteffan/Zotero/storage/L8AWPMVZ/Goeke et al. - 2017 - LabVanced A Unified JavaScript Framework for Onli.pdf}
}

@online{hanzhang[@_hanzhang_]WebcambasedEyetrackerWebgazer2022,
  type = {Tweet},
  title = {Is the Webcam-Based Eye-Tracker "{{Webgazer}}" Any Good for Research? {{I}} Compared {{Webgazer}} against {{Eyelink}} with Concurrent Recording. {{Left}}: {{Eyelink}} 1000. {{Right}}: {{Webgazer}}. {{That}}'s .6deg/10mm vs. 3deg/50mm in Terms of Accuracy😬. {{https://t.co/9Zm0J1Xfur}}},
  author = {{Han Zhang [@\_HanZhang\_]}},
  date = {2022-05-20T21:25Z},
  url = {https://twitter.com/_HanZhang_/status/1527762360076738560},
  urldate = {2022-10-31},
  langid = {english},
  organization = {{Twitter}},
  file = {/Users/adriansteffan/Zotero/storage/WVFEK26R/1527762360076738560.html}
}

@article{harezlakAccurateEyeTracker2014,
  title = {Towards {{Accurate Eye Tracker Calibration}} – {{Methods}} and {{Procedures}}},
  author = {Harezlak, Katarzyna and Kasprowski, Pawel and Stasch, Mateusz},
  date = {2014-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}} 18th {{Annual Conference}}, {{KES-2014 Gdynia}}, {{Poland}}, {{September}} 2014 {{Proceedings}}},
  volume = {35},
  pages = {1073--1081},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2014.08.194},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050914011594},
  urldate = {2021-09-07},
  abstract = {Eye movement is a new emerging modality in human computer interfaces. With better access to devices which are able to measure eye movements (so called eye trackers) it becomes accessible even in ordinary environments. However, the first problem that must be faced when working with eye movements is a correct mapping from an output of eye tracker to a gaze point – place where the user is looking at the screen. That is why the work must always be started with calibration of the device. The paper describes the process of calibration, analyses of the possible steps and ways how to simplify this process.},
  langid = {english},
  keywords = {calibration,eye movement,eye trackers,regression},
  file = {/Users/adriansteffan/Zotero/storage/2UCRV3MU/2UCRV3MU.pdf;/Users/adriansteffan/Zotero/storage/EAQNLLN3/S1877050914011594.html}
}

@online{krafkaEyeTrackingEveryone2016,
  title = {Eye {{Tracking}} for {{Everyone}}},
  author = {Krafka, Kyle and Khosla, Aditya and Kellnhofer, Petr and Kannan, Harini and Bhandarkar, Suchendra and Matusik, Wojciech and Torralba, Antonio},
  date = {2016-06-18},
  eprint = {1606.05814},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.05814},
  url = {http://arxiv.org/abs/1606.05814},
  urldate = {2023-07-03},
  abstract = {From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/adriansteffan/Zotero/storage/CKCR3X3R/Krafka et al. - 2016 - Eye Tracking for Everyone.pdf;/Users/adriansteffan/Zotero/storage/BSN8X7TZ/1606.html}
}

@article{oakesAdvancesEyeTracking2012,
  title = {Advances in {{Eye Tracking}} in {{Infancy Research}}},
  author = {Oakes, Lisa M.},
  date = {2012},
  journaltitle = {Infancy},
  volume = {17},
  number = {1},
  pages = {1--8},
  issn = {1532-7078},
  doi = {10.1111/j.1532-7078.2011.00101.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1532-7078.2011.00101.x},
  urldate = {2023-07-03},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/J7YMKYDH/j.1532-7078.2011.00101.html}
}

@article{papoutsakiWebGazerScalableWebcam2016,
  title = {{{WebGazer}}: {{Scalable Webcam Eye Tracking Using User Interactions}}},
  author = {Papoutsaki, Alexandra and Sangkloy, Patsorn and Laskey, James and Daskalova, Nediyana and Huang, Jeff and Hays, James},
  date = {2016},
  pages = {7},
  abstract = {We introduce WebGazer, an online eye tracker that uses common webcams already present in laptops and mobile devices to infer the eye-gaze locations of web visitors on a page in real time. The eye tracking model self-calibrates by watching web visitors interact with the web page and trains a mapping between features of the eye and positions on the screen. This approach aims to provide a natural experience to everyday users that is not restricted to laboratories and highly controlled user studies. WebGazer has two key components: a pupil detector that can be combined with any eye detection library, and a gaze estimator using regression analysis informed by user interactions. We perform a large remote online study and a small in-person study to evaluate WebGazer. The findings show that WebGazer can learn from user interactions and that its accuracy is sufficient for approximating the user’s gaze. As part of this paper, we release the first eye tracking library that can be easily integrated in any website for real-time gaze interactions, usability studies, or web research.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/PJRBPCJV/Papoutsaki et al. - WebGazer Scalable Webcam Eye Tracking Using User .pdf}
}

@online{preinTANGOReliableOpensource2022,
  title = {{{TANGO}}: {{A}} Reliable, Open-Source, Browser-Based Task to Assess Individual Differences in Gaze Understanding in 3 to 5-Year-Old Children and Adults},
  shorttitle = {{{TANGO}}},
  author = {Prein, Julia Christin and Bohn, Manuel and Kalinke, Steven and Haun, Daniel B. M.},
  date = {2022-09-08T07:51:37},
  doi = {10.31234/osf.io/vghw8},
  url = {https://psyarxiv.com/vghw8/},
  urldate = {2023-07-03},
  abstract = {Traditional measures of social cognition used in developmental research often lack satisfactory psychometric properties and are not designed to capture variation between individuals. Here we present TANGO (Task for Assessing iNdividual differences in Gaze understanding - Open); a brief (approx. 5–10min), reliable, open-source task to quantify individual differences in the understanding of gaze cues. Our interactive browser-based task works across devices and enables in-person and remote testing. The implemented spatial layout allows for discrete and continuous measures of participants’ click imprecision and is easily adaptable to different study requirements. Our task measures inter-individual differences in a child (N = 387) and an adult (N = 236) sample. Our two study versions and data collection modes yield comparable results that show substantial developmental gains: the older children are, the more accurately they locate the target. High internal consistency and test–retest reliability estimates underline that the captured variation is systematic. This work shows a promising way forward in studying individual differences in social cognition and will help us explore the structure and development of our core social-cognitive processes in greater detail.},
  langid = {american},
  pubstate = {preprint},
  keywords = {cognitive development,Cognitive Development,Developmental Psychology,gaze cues,individual differences,Social and Behavioral Sciences,social cognition},
  file = {/Users/adriansteffan/Zotero/storage/WTVHG465/Prein et al. - 2022 - TANGO A reliable, open-source, browser-based task.pdf}
}

@manual{R-base,
  type = {manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2021},
  publisher = {{R Foundation for Statistical Computing}},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/}
}

@manual{R-papaja,
  type = {manual},
  title = {{{papaja}}: {{Prepare}} Reproducible {{APA}} Journal Articles with {{R Markdown}}},
  author = {Aust, Frederik and Barth, Marius},
  date = {2022},
  url = {https://github.com/crsh/papaja}
}

@inproceedings{saxenaEfficientCalibrationWebcam2022,
  title = {Towards Efficient Calibration for Webcam Eye-Tracking in Online Experiments},
  booktitle = {2022 {{Symposium}} on {{Eye Tracking Research}} and {{Applications}}},
  author = {Saxena, Shreshth and Lange, Elke and Fink, Lauren},
  date = {2022-06-08},
  series = {{{ETRA}} '22},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3517031.3529645},
  url = {https://doi.org/10.1145/3517031.3529645},
  urldate = {2022-10-31},
  abstract = {Calibration is performed in eye-tracking studies to map raw model outputs to gaze-points on the screen and improve accuracy of gaze predictions. Calibration parameters, such as user-screen distance, camera intrinsic properties, and position of the screen with respect to the camera can be easily calculated in controlled offline setups, however, their estimation is non-trivial in unrestricted, online, experimental settings. Here, we propose the application of deep learning models for eye-tracking in online experiments, providing suitable strategies to estimate calibration parameters and perform personal gaze calibration. Focusing on fixation accuracy, we compare results with respect to calibration frequency, the time point of calibration during data collection (beginning, middle, end), and calibration procedure (fixation-point or smooth pursuit-based). Calibration using fixation and smooth pursuit tasks, pooled over three collection time-points, resulted in the best fixation accuracy. By combining device calibration, gaze calibration, and the best-performing deep-learning model, we achieve an accuracy of 2.580−a considerable improvement over reported accuracies in previous online eye-tracking studies.},
  isbn = {978-1-4503-9252-5},
  keywords = {deep learning,gaze calibration,online experiments,webcam eye-tracking},
  file = {/Users/adriansteffan/Zotero/storage/YN6XXK7V/Saxena et al. - 2022 - Towards efficient calibration for webcam eye-track.pdf}
}

@article{schidelkoOnlineTestingYields2021,
  title = {Online {{Testing Yields}} the {{Same Results}} as {{Lab Testing}}: {{A Validation Study With}} the {{False Belief Task}}},
  shorttitle = {Online {{Testing Yields}} the {{Same Results}} as {{Lab Testing}}},
  author = {Schidelko, Lydia Paulin and Schünemann, Britta and Rakoczy, Hannes and Proft, Marina},
  date = {2021},
  journaltitle = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.703238},
  urldate = {2023-07-03},
  abstract = {Recently, online testing has become an increasingly important instrument in developmental research, in particular since the COVID-19 pandemic made in-lab testing impossible. However, online testing comes with two substantial challenges. First, it is unclear how valid results of online studies really are. Second, implementing online studies can be costly and/or require profound coding skills. This article addresses the validity of an online testing approach that is low-cost and easy to implement: The experimenter shares test materials such as videos or presentations via video chat and interactively moderates the test session. To validate this approach, we compared children’s performance on a well-established task, the change-of-location false belief task, in an in-lab and online test setting. In two studies, 3- and 4-year-old received online implementations of the false belief version (Study 1) and the false and true belief version of the task (Study 2). Children’s performance in these online studies was compared to data of matching tasks collected in the context of in-lab studies. Results revealed that the typical developmental pattern of performance in these tasks found in in-lab studies could be replicated with the novel online test procedure. These results suggest that the proposed method, which is both low-cost and easy to implement, provides a valid alternative to classical in-person test settings.},
  file = {/Users/adriansteffan/Zotero/storage/WJZLLCWL/Schidelko et al. - 2021 - Online Testing Yields the Same Results as Lab Test.pdf}
}

@article{schlegelmilchEffectsCalibrationTarget2019,
  title = {The {{Effects}} of {{Calibration Target}}, {{Screen Location}}, and {{Movement Type}} on {{Infant Eye-Tracking Data Quality}}},
  author = {Schlegelmilch, Karola and Wertz, Annie E.},
  date = {2019-07},
  journaltitle = {Infancy: The Official Journal of the International Society on Infant Studies},
  shortjournal = {Infancy},
  volume = {24},
  number = {4},
  eprint = {32677249},
  eprinttype = {pmid},
  pages = {636--662},
  issn = {1532-7078},
  doi = {10.1111/infa.12294},
  abstract = {During infant eye-tracking, fussiness caused by the repetition of calibration stimuli and body movements during testing are frequent constraints on measurement quality. Here, we systematically investigated these constraints with infants and adults using EyeLink 1000 Plus. We compared looking time and dispersion of gaze points elicited by stimuli resembling commonly used calibration animations. The adult group additionally performed body movements during gaze recording that were equivalent to movements infants spontaneously produce during testing. In our results, infants' preference for a particular calibration target did not predict data quality elicited by that stimulus, but targets exhibiting the strongest contrasts in their center or targets with globally distributed complexity resulted in the highest accuracy. Our gaze measures from the adult movement tasks were differentially affected by the type of movement as well as the location where the target appeared on the screen. These heterogeneous effects of movement on measures should be taken into account when planning infant eye-tracking experiments. Additionally, to improve data quality, infants' tolerance for repeated calibrations can be facilitated by alternating between precise calibration targets.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/J9Q9DB7E/Schlegelmilch and Wertz - 2019 - The Effects of Calibration Target, Screen Location.pdf}
}

@online{schuwerkActionAnticipationBased2022,
  title = {Action Anticipation Based on an Agent's Epistemic State in Toddlers and Adults},
  author = {Schuwerk, Tobias and Kampis, Dora and Baillargeon, Renée and Biro, Szilvia and Bohn, Manuel and Byers-Heinlein, Krista and Dörrenberg, Sebastian and Fisher, Cynthia and Franchin, Laura and Fulcher, Tess and Garbisch, Isa and Geraci, Alessandra and Wiesmann, Charlotte Grosse and Hamlin, Kiley and Haun, Daniel B. M. and Hepach, Robert and Hunnius, Sabine and Hyde, Daniel C. and Karman, Petra and Kosakowski, Heather and Kovács, Ágnes M. and Krämer, Anna and Kulke, Louisa and Lee, Crystal and Lew-Williams, Casey and Liszkowski, Ulf and Mahowald, Kyle and Mascaro, Olivier and Meyer, Marlene and Moreau, David and Perner, Josef and Poulin-Dubois, Diane and Powell, Lindsey and Prein, Julia Christin and Priewasser, Beate and Proft, Marina and Raz, Gal and Reschke, Peter Joseph and Ross, Josephine and Rothmaler, Katrin and Saxe, Rebecca and Schneider, Dana and Southgate, Victoria and Surian, Luca and Tebbe, Anna-Lena and Träuble, Birgit and Tsui, Angeline and Wertz, Annie and Woodward, Amanda and Yuen, Francis and Yuile, Amanda Rose and Zellner, Luise and Zimmer, Lucie and Frank, Michael C. and Rakoczy, Hannes},
  date = {2022-02-13T15:08:31},
  doi = {10.31234/osf.io/x4jbm},
  url = {https://psyarxiv.com/x4jbm/},
  urldate = {2023-07-03},
  abstract = {Do toddlers and adults engage in spontaneous Theory of Mind (ToM)? Evidence from anticipatory looking (AL) studies suggests that they do. But a growing body of failed replication studies raised questions about the paradigm’s suitability. In this multi-lab collaboration, we test the robustness of spontaneous ToM measures. We examine whether 18- to 27-month-olds’ and adults’ anticipatory looks distinguish between two basic forms of an agent’s epistemic states: knowledge and ignorance. In toddlers [ANTICIPATED n = 520 50\% FEMALE] and adults [ANTICIPATED n = 408, 50\% FEMALE] from diverse ethnic backgrounds, we found [SUPPORT/NO SUPPORT] for epistemic state-based action anticipation. Future research can probe whether this conclusion extends to more complex kinds of epistemic states, such as true and false beliefs.},
  langid = {american},
  pubstate = {preprint},
  keywords = {anticipatory looking,Cognitive Development,Developmental Psychology,replication,Social and Behavioral Sciences,spontaneous Theory of Mind},
  file = {/Users/adriansteffan/Zotero/storage/JBS62FQF/Schuwerk et al. - 2021 - Action anticipation based on an agent's epistemic .pdf}
}

@article{scottLookitPartNew2017a,
  title = {Lookit ({{Part}} 1): {{A New Online Platform}} for {{Developmental Research}}},
  shorttitle = {Lookit ({{Part}} 1)},
  author = {Scott, Kimberly and Schulz, Laura},
  date = {2017-02-01},
  journaltitle = {Open Mind},
  shortjournal = {Open Mind},
  volume = {1},
  number = {1},
  pages = {4--14},
  issn = {2470-2986},
  doi = {10.1162/OPMI_a_00002},
  url = {https://doi.org/10.1162/OPMI_a_00002},
  urldate = {2023-07-03},
  abstract = {Many important questions about children’s early abilities and learning mechanisms remain unanswered not because of their inherent scientific difficulty but because of practical challenges: recruiting an adequate number of children, reaching special populations, or scheduling repeated sessions. Additionally, small participant pools create barriers to replication while differing laboratory environments make it difficult to share protocols with precision, limiting the reproducibility of developmental research. Here we introduce a new platform, “Lookit,” that addresses these constraints by allowing families to participate in behavioral studies online via webcam. We show that this platform can be used to test infants (11–18 months), toddlers (24–36 months), and preschoolers (36–60 months) and reliably code looking time, preferential looking, and verbal responses, respectively; empirical results of these studies are presented in Scott, Chu, and Schulz (2017). In contrast to most laboratory-based studies, participants were roughly representative of the American population with regards to income, race, and parental education. We discuss broad technical and methodological aspects of the platform, its strengths and limitations, recommendations for researchers interested in conducting developmental studies online, and issues that remain before online testing can fulfill its promise.},
  file = {/Users/adriansteffan/Zotero/storage/4CXBRLYL/Scott and Schulz - 2017 - Lookit (Part 1) A New Online Platform for Develop.pdf;/Users/adriansteffan/Zotero/storage/PQP864YE/Lookit-Part-1-A-New-Online-Platform-for.html}
}

@article{semmelmannOnlineWebcambasedEye2018,
  title = {Online Webcam-Based Eye Tracking in Cognitive Science: {{A}} First Look},
  shorttitle = {Online Webcam-Based Eye Tracking in Cognitive Science},
  author = {Semmelmann, Kilian and Weigelt, Sarah},
  date = {2018-04-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {50},
  number = {2},
  pages = {451--465},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0913-7},
  url = {https://doi.org/10.3758/s13428-017-0913-7},
  urldate = {2021-05-08},
  abstract = {Online experimentation is emerging in many areas of cognitive psychology as a viable alternative or supplement to classical in-lab experimentation. While performance- and reaction-time-based paradigms are covered in recent studies, one instrument of cognitive psychology has not received much attention up to now: eye tracking. In this study, we used JavaScript-based eye tracking algorithms recently made available by Papoutsaki et al. (International Joint Conference on Artificial Intelligence, 2016) together with consumer-grade webcams to investigate the potential of online eye tracking to benefit from the common advantages of online data conduction. We compared three in-lab conducted tasks (fixation, pursuit, and free viewing) with online-acquired data to analyze the spatial precision in the first two, and replicability of well-known gazing patterns in the third task. Our results indicate that in-lab data exhibit an offset of about 172 px (15\% of screen size, 3.94° visual angle) in the fixation task, while online data is slightly less accurate (18\% of screen size, 207 px), and shows higher variance. The same results were found for the pursuit task with a constant offset during the stimulus movement (211 px in-lab, 216 px online). In the free-viewing task, we were able to replicate the high attention attribution to eyes (28.25\%) compared to other key regions like the nose (9.71\%) and mouth (4.00\%). Overall, we found web technology-based eye tracking to be suitable for all three tasks and are confident that the required hard- and software will be improved continuously for even more sophisticated experimental paradigms in all of cognitive psychology.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/5BZJV34C/Semmelmann and Weigelt - 2018 - Online webcam-based eye tracking in cognitive scie.pdf}
}

@inproceedings{spakovEnablingUnsupervisedEye2018,
  title = {Enabling Unsupervised Eye Tracker Calibration by School Children through Games},
  booktitle = {Proceedings of the 2018 {{ACM Symposium}} on {{Eye Tracking Research}} \& {{Applications}}},
  author = {Špakov, Oleg and Istance, Howell and Viitanen, Tiia and Siirtola, Harri and Räihä, Kari-Jouko},
  date = {2018-06-14},
  series = {{{ETRA}} '18},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3204493.3204534},
  url = {https://doi.org/10.1145/3204493.3204534},
  urldate = {2023-07-03},
  abstract = {To use eye trackers in a school classroom, children need to be able to calibrate their own tracker unsupervised and on repeated occasions. A game designed specifically around the need to maintain their gaze in fixed locations was used to collect calibration and verification data. The data quality obtained was compared with a standard calibration procedure and another game, in two studies carried out in three elementary schools. One studied the effect on data quality over repeated occasions and the other studied the effect of age on data quality. The first showed that accuracy obtained from unsupervised calibration by children was twice as good after six occasions with the game requiring the fixed gaze location compared with the standard calibration, and as good as standard calibration by group of supervised adults. In the second study, age was found to have no effect on performance in the groups of children studied.},
  isbn = {978-1-4503-5706-7},
  keywords = {calibration,games,low cost tracker,school children,unsupervised},
  file = {/Users/adriansteffan/Zotero/storage/SDRSZX2T/Špakov et al. - 2018 - Enabling unsupervised eye tracker calibration by s.pdf}
}

@book{steffanValidationOpenSource2023,
  title = {Validation of an {{Open Source}}, {{Remote Web-based Eye-tracking Method}} ({{WebGazer}}) for {{Research}} in {{Early Childhood}}},
  author = {Steffan, Adrian and Zimmer, Lucie and Arias-Trejo, Natalia and Bohn, Manuel and Dal Ben, Rodrigo and Flores-Coronado, Marco and Franchin, Laura and Garbisch, Isa and Grosse Wiesmann, Charlotte and Hamlin, J Kiley and Havron, Naomi and Hay, Jessica and Hermansen, Tone and Jakobsen, Krisztina and Kalinke, Steven and Ko, Eon-Suk and Kulke, Louisa and Mayor, Julien and Meristo, Marek and Schuwerk, Tobias},
  date = {2023-01-23},
  doi = {10.31234/osf.io/7924h},
  abstract = {Measuring eye movements remotely via the participant’s webcam promises to be an attractive methodological addition to in-person eye-tracking in the lab. However, there is a lack of systematic research comparing remote web-based eye-tracking with in-lab eye-tracking in young children. We report a multi-lab study that compared these two measures in an anticipatory looking task with toddlers using WebGazer.js and jsPsych. Results of our remotely tested sample of 18-27-month-old toddlers (N = 125) revealed that web-based eye-tracking successfully captured goal-based action predictions, although the proportion of the goal-directed anticipatory looking was lower compared to the in-lab sample (N = 70). As expected, attrition rate was substantially higher in the web-based (42\%) than the in-lab sample (10\%). Excluding trials based on visual inspection of the match of time-locked gaze coordinates and the participant’s webcam video overlayed on the stimuli was an important preprocessing step to reduce noise in the data. We discuss the use of this remote web-based method in comparison with other current methodological innovations. Our study demonstrates that remote web-based eye-tracking can be a useful tool for testing toddlers, facilitating recruitment of larger and more diverse samples; a caveat to consider is the larger drop-out rate.},
  file = {/Users/adriansteffan/Zotero/storage/5I68VPR6/Steffan et al. - 2023 - Validation of an Open Source, Remote Web-based Eye.pdf}
}

@software{thephpgroupPHP2020,
  title = {{{PHP}}},
  author = {{The PHP Group}},
  date = {2020},
  url = {https://www.php.net/},
  version = {8.0}
}

@article{valliappanAcceleratingEyeMovement2020,
  title = {Accelerating Eye Movement Research via Accurate and Affordable Smartphone Eye Tracking},
  author = {Valliappan, Nachiappan and Dai, Na and Steinberg, Ethan and He, Junfeng and Rogers, Kantwon and Ramachandran, Venky and Xu, Pingmei and Shojaeizadeh, Mina and Guo, Li and Kohlhoff, Kai and Navalpakkam, Vidhya},
  date = {2020-09-11},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {4553},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-18360-5},
  url = {https://www.nature.com/articles/s41467-020-18360-5},
  urldate = {2021-09-07},
  abstract = {Eye tracking has been widely used for decades in vision research, language and usability. However, most prior research has focused on large desktop displays using specialized eye trackers that are expensive and cannot scale. Little is known about eye movement behavior on phones, despite their pervasiveness and large amount of time spent. We leverage machine learning to demonstrate accurate smartphone-based eye tracking without any additional hardware. We show that the accuracy of our method is comparable to state-of-the-art mobile eye trackers that are 100x more expensive. Using data from over 100 opted-in users, we replicate key findings from previous eye movement research on oculomotor tasks and saliency analyses during natural image viewing. In addition, we demonstrate the utility of smartphone-based gaze for detecting reading comprehension difficulty. Our results show the potential for scaling eye movement research by orders-of-magnitude to thousands of participants (with explicit consent), enabling advances in vision research, accessibility and healthcare.},
  issue = {1},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Attention;Human behaviour;Visual system Subject\_term\_id: attention;human-behaviour;visual-system},
  file = {/Users/adriansteffan/Zotero/storage/YT27BHI3/YT27BHI3.pdf;/Users/adriansteffan/Zotero/storage/982JR6FA/s41467-020-18360-5.html}
}

@article{venkerOpenConversationUsing2015,
  title = {An {{Open Conversation}} on {{Using Eye-Gaze Methods}} in {{Studies}} of {{Neurodevelopmental Disorders}}},
  author = {Venker, Courtney E. and Kover, Sara T.},
  date = {2015-12},
  journaltitle = {Journal of speech, language, and hearing research: JSLHR},
  shortjournal = {J Speech Lang Hear Res},
  volume = {58},
  number = {6},
  eprint = {26363412},
  eprinttype = {pmid},
  pages = {1719--1732},
  issn = {1558-9102},
  doi = {10.1044/2015_JSLHR-L-14-0304},
  abstract = {PURPOSE: Eye-gaze methods have the potential to advance the study of neurodevelopmental disorders. Despite their increasing use, challenges arise in using these methods with individuals with neurodevelopmental disorders and in reporting sufficient methodological detail such that the resulting research is replicable and interpretable. METHOD: This tutorial presents key considerations involved in designing and conducting eye-gaze studies for individuals with neurodevelopmental disorders and proposes conventions for reporting the results of such studies. RESULTS: Methodological decisions (e.g., whether to use automated eye tracking or manual coding, implementing strategies to scaffold children's performance, defining valid trials) have cascading effects on the conclusions drawn from eye-gaze data. Research reports that include specific information about procedures, missing data, and selection of participants will facilitate interpretation and replication. CONCLUSIONS: Eye-gaze methods provide exciting opportunities for studying neurodevelopmental disorders. Open discussion of the issues presented in this tutorial will improve the pace of productivity and the impact of advances in research on neurodevelopmental disorders.},
  langid = {english},
  pmcid = {PMC4987028},
  keywords = {Biomedical Research,Child,Eye Movement Measurements,Eye Movements,Humans,Neurodevelopmental Disorders,{Pattern Recognition, Automated},Psychological Tests,Reproducibility of Results,Research Design,Time Factors},
  file = {/Users/adriansteffan/Zotero/storage/7JTVYNBP/Venker and Kover - 2015 - An Open Conversation on Using Eye-Gaze Methods in .pdf}
}

@article{visserImprovingGeneralizabilityInfant2021,
  title = {Improving the Generalizability of Infant Psychological Research: {{The ManyBabies}} Model},
  shorttitle = {Improving the Generalizability of Infant Psychological Research},
  author = {Visser, Ingmar and Bergmann, Christina and Byers-Heinlein, Krista and Ben, Rodrigo Dal and Duch, Wlodzislaw and Forbes, Samuel H. and Franchin, Laura and Frank, Michael C. and Geraci, Alessandra and Hamlin, Kiley and Kaldy, Zsuzsa and Kulke, Louisa and Laverty, Catherine and Lew-Williams, Casey and Mateu, Victoria and Mayor, Julien and Moreau, David and Nomikou, Iris and Schuwerk, Tobias and Simpson, Elizabeth and Singh, Leher and Soderstrom, Melanie and Sullivan, Jess and family=Heuvel, given=Marion I., prefix=van den, useprefix=false and Westermann, Gert and Yamada, Yuki and Zaadnoordijk, Lorijn and Zettersten, Martin},
  date = {2021-04-02T08:02:19},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/8vwbf},
  url = {https://psyarxiv.com/8vwbf/},
  urldate = {2021-08-26},
  abstract = {Yarkoni’s analysis clearly articulates a number of concerns limiting the generalizability and explanatory power of psychological findings, many of which are compounded in infancy research. ManyBabies addresses these concerns via a radically collaborative, large-scale and open approach to research that is grounded in theory-building, committed to diversification, and focused on understanding sources of variation.},
  keywords = {Developmental Psychology,Infancy,infant research,manybabies,Meta-science,open science,Social and Behavioral Sciences},
  file = {/Users/adriansteffan/Zotero/storage/WXJUQD8G/Visser et al. - 2021 - Improving the generalizability of infant psycholog.pdf}
}

@article{visserImprovingGeneralizabilityInfant2022,
  title = {Improving the Generalizability of Infant Psychological Research: {{The ManyBabies}} Model},
  shorttitle = {Improving the Generalizability of Infant Psychological Research},
  author = {Visser, Ingmar and Bergmann, Christina and Byers-Heinlein, Krista and Ben, Rodrigo Dal and Duch, Wlodzislaw and Forbes, Samuel and Franchin, Laura and Frank, Michael C. and Geraci, Alessandra and Hamlin, J. Kiley and Kaldy, Zsuzsa and Kulke, Louisa and Laverty, Catherine and Lew-Williams, Casey and Mateu, Victoria and Mayor, Julien and Moreau, David and Nomikou, Iris and Schuwerk, Tobias and Simpson, Elizabeth A. and Singh, Leher and Soderstrom, Melanie and Sullivan, Jessica and family=Heuvel, given=Marion I., prefix=van den, useprefix=false and Westermann, Gert and Yamada, Yuki and Zaadnoordijk, Lorijn and Zettersten, Martin},
  date = {2022-01},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e35},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X21000455},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/improving-the-generalizability-of-infant-psychological-research-the-manybabies-model/B56EA6B4115B814F4430367E1ABF2DDC},
  urldate = {2023-07-03},
  abstract = {Yarkoni's analysis clearly articulates a number of concerns limiting the generalizability and explanatory power of psychological findings, many of which are compounded in infancy research. ManyBabies addresses these concerns via a radically collaborative, large-scale and open approach to research that is grounded in theory-building, committed to diversification, and focused on understanding sources of variation.},
  langid = {english},
  file = {/Users/adriansteffan/Zotero/storage/8X239ACR/Visser et al. - 2022 - Improving the generalizability of infant psycholog.pdf}
}

@article{werchanOWLETAutomatedOpensource2022,
  title = {{{OWLET}}: {{An}} Automated, Open-Source Method for Infant Gaze Tracking Using Smartphone and Webcam Recordings},
  shorttitle = {{{OWLET}}},
  author = {Werchan, Denise M. and Thomason, Moriah E. and Brito, Natalie H.},
  date = {2022-09-07},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-022-01962-w},
  url = {https://doi.org/10.3758/s13428-022-01962-w},
  urldate = {2022-10-31},
  abstract = {Groundbreaking insights into the origins of the human mind have been garnered through the study of eye movements in preverbal subjects who are unable to explain their thought processes. Developmental research has largely relied on in-lab testing with trained experimenters. This constraint provides a narrow window into infant cognition and impedes large-scale data collection in families from diverse socioeconomic, geographic, and cultural backgrounds. Here we introduce a new open-source methodology for automatically analyzing infant eye-tracking data collected on personal devices in the home. Using algorithms from computer vision, machine learning, and ecological psychology, we develop an online webcam-linked eye tracker (OWLET) that provides robust estimation of infants’ point of gaze from smartphone and webcam recordings of infant assessments in the home. We validate OWLET in a large sample of 7-month-old infants (N = 127) tested remotely, using an established visual attention task. We show that this new method reliably estimates infants’ point-of-gaze across a variety of contexts, including testing on both computers and mobile devices, and exhibits excellent external validity with parental-report measures of attention. Our platform fills a significant gap in current tools available for rapid online data collection and large-scale assessments of cognitive processes in infants. Remote assessment addresses the need for greater diversity and accessibility in human studies and may support the ecological validity of behavioral experiments. This constitutes a critical and timely advance in a core domain of developmental research and in psychological science more broadly.},
  langid = {english},
  keywords = {Development,Infancy,Online studies,Webcam eye tracking},
  file = {/Users/adriansteffan/Zotero/storage/7PRIKDHY/Werchan et al. - 2022 - OWLET An automated, open-source method for infant.pdf}
}

@article{yangWebcambasedOnlineEyetracking2021,
  title = {Webcam-Based Online Eye-Tracking for Behavioral Research},
  author = {Yang, Xiaozhi and Krajbich, Ian},
  date = {2021-11},
  journaltitle = {Judgment and Decision Making},
  volume = {16},
  number = {6},
  pages = {1485--1505},
  publisher = {{Cambridge University Press}},
  issn = {1930-2975},
  doi = {10.1017/S1930297500008512},
  url = {https://www.cambridge.org/core/journals/judgment-and-decision-making/article/webcambased-online-eyetracking-for-behavioral-research/B726E77B68A76577F9BC6BB8F1EBC6E4},
  urldate = {2023-07-03},
  abstract = {Experiments are increasingly moving online. This poses a major challenge for researchers who rely on in-lab techniques such as eye-tracking. Researchers in computer science have developed web-based eye-tracking applications (WebGazer; Papoutsaki et al., 2016) but they have yet to see them used in behavioral research. This is likely due to the extensive calibration and validation procedure, inconsistent temporal resolution (Semmelmann \& Weigelt, 2018), and the challenge of integrating it into experimental software. Here, we incorporate WebGazer into a JavaScript library widely used by behavioral researchers (jsPsych) and adjust the procedure and code to reduce calibration/validation and improve the temporal resolution (from 100–1000 ms to 20–30 ms). We test this procedure with a decision-making study on Amazon MTurk, replicating previous in-lab findings on the relationship between gaze and choice, with little degradation in spatial or temporal resolution. This provides evidence that online web-based eye-tracking is feasible in behavioral research.},
  langid = {english},
  keywords = {attention,attentional drift diffusion model,decision-making,eye-tracking,online studies,preferences},
  file = {/Users/adriansteffan/Zotero/storage/CAV5NCUE/Yang and Krajbich - 2021 - Webcam-based online eye-tracking for behavioral re.pdf}
}

@article{zengMaximizingValidEyetracking2023,
  title = {Maximizing Valid Eye-Tracking Data in Human and Macaque Infants by Optimizing Calibration and Adjusting Areas of Interest},
  author = {Zeng, Guangyu and Simpson, Elizabeth A. and Paukner, Annika},
  date = {2023-03-08},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res Methods},
  eprint = {36890330},
  eprinttype = {pmid},
  issn = {1554-3528},
  doi = {10.3758/s13428-022-02056-3},
  abstract = {Remote eye tracking with automated corneal reflection provides insights into the emergence and development of cognitive, social, and emotional functions in human infants and non-human primates. However, because most eye-tracking systems were designed for use in human adults, the accuracy of eye-tracking data collected in other populations is unclear, as are potential approaches to minimize measurement error. For instance, data quality may differ across species or ages, which are necessary considerations for comparative and developmental studies. Here we examined how the calibration method and adjustments to areas of interest (AOIs) of the Tobii TX300 changed the mapping of fixations to AOIs in a cross-species longitudinal study. We tested humans (N = 119) at 2, 4, 6, 8, and 14 months of age and macaques (Macaca mulatta; N = 21) at 2 weeks, 3 weeks, and 6 months of age. In all groups, we found improvement in the proportion of AOI hits detected as the number of successful calibration points increased, suggesting calibration approaches with more points may be advantageous. Spatially enlarging and temporally prolonging AOIs increased the number of fixation-AOI mappings, suggesting improvements in capturing infants' gaze behaviors; however, these benefits varied across age groups and species, suggesting different parameters may be ideal, depending on the population studied. In sum, to maximize usable sessions and minimize measurement error, eye-tracking data collection and extraction approaches may need adjustments for the age groups and species studied. Doing so may make it easier to standardize and replicate eye-tracking research findings.},
  langid = {english},
  keywords = {Comparative psychology,Developmental psychology,Eye gaze,Infancy,Measurement,Orienting,Vision,Visual attention}
}
